{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "trainpath = 'data/train-en-es.csv'\n",
    "testpath = 'data/test-en-es.csv'\n",
    "valpath = 'data/validation-en-es.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath)\n",
    "testdata = pd.read_csv(testpath)\n",
    "valdata = pd.read_csv(valpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained Word2Vec model <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_model = KeyedVectors.load_word2vec_format(\"./data/SBW-vectors-300-min5.bin.gz\", binary=True)\n",
    "en_model = KeyedVectors.load_word2vec_format(\"/home/ishitbansal/Semester-6/INLP/Project/Advanced_Semantic_similarity-main/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding_es(sentence):\n",
    "    words = sentence\n",
    "    unk_token = \"unk\"\n",
    "    words = [word if word in es_model.key_to_index else unk_token for word in words]\n",
    "    if len(words) == 0:\n",
    "        words = [\"unk\"]\n",
    "    embeddings = [es_model[word] for word in words]\n",
    "    embedding = np.mean(embeddings, axis=0)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_sentence_embedding_en(sentence):\n",
    "    words = sentence\n",
    "    unk_token = \"unk\"\n",
    "    words = [word if word in en_model.key_to_index else unk_token for word in words]\n",
    "    if len(words) == 0:\n",
    "        words = [\"unk\"]\n",
    "    embeddings = [en_model[word] for word in words]\n",
    "    embedding = np.mean(embeddings, axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = np.array([get_sentence_embedding_en(sentence) for sentence in traindata['sentence1']])\n",
    "x_train2 = np.array([get_sentence_embedding_en(sentence) for sentence in traindata['sentence2']])\n",
    "y_train = list(traindata['similarity_score'])\n",
    "\n",
    "x_val1 = np.array([get_sentence_embedding_en(sentence) for sentence in valdata['sentence1']])\n",
    "x_val2 = np.array([get_sentence_embedding_es(sentence) for sentence in valdata['sentence2']])\n",
    "y_val = list(valdata['similarity_score'])\n",
    "\n",
    "x_test1 = np.array([get_sentence_embedding_en(sentence) for sentence in testdata['sentence1']])\n",
    "x_test2 = np.array([get_sentence_embedding_es(sentence) for sentence in testdata['sentence2']])\n",
    "y_test = list(testdata['similarity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSimilarityDataset(data.Dataset):\n",
    "    def __init__(self, embeddings1, embeddings2, scores):\n",
    "        self.embeddings1 = embeddings1\n",
    "        self.embeddings2 = embeddings2\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.embeddings1), len(self.embeddings2))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.embeddings1[index]), torch.tensor(self.embeddings2[index]), torch.tensor(self.scores[index], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 300 \n",
    "hidden_dim = 150\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "trainset = SentenceSimilarityDataset(x_train1, x_train2, y_train)\n",
    "valset = SentenceSimilarityDataset(x_val1, x_val2, y_val)\n",
    "testset = SentenceSimilarityDataset(x_test1, x_test2, y_test)\n",
    "\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "testloader = data.DataLoader(testset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.dropout(out) \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class GRURegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, attention_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_dim = attention_dim\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = SelfAttention(hidden_dim*2, attention_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        att_weights = self.attention(out)\n",
    "        out = torch.sum(out * att_weights, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, attention_input):\n",
    "        output = self.layer1(attention_input)\n",
    "        output = torch.tanh(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs, train_dataloader,val_dataloader):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for embeddings1_batch, embeddings2_batch, scores_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(embeddings1_batch, embeddings2_batch)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            loss = loss_fn(output.squeeze(), scores_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(embeddings1_batch)\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_embeddings1_batch, val_embeddings2_batch, val_scores_batch in val_dataloader:\n",
    "                val_output = model(val_embeddings1_batch, val_embeddings2_batch)\n",
    "                val_loss += loss_fn(val_output.squeeze(), val_scores_batch).item() * len(val_embeddings1_batch)\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {} - Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, train_loss, val_loss))\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    y_pred_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for emb1, emb2, scores in data_loader:\n",
    "            test_output = model(emb1, emb2)\n",
    "            y_pred_test.extend(test_output.squeeze().tolist())\n",
    "    return y_pred_test\n",
    "\n",
    "def print_values(model, loader, y_true):\n",
    "    y_pred = evaluate(model, loader)\n",
    "    corr = pearson_corr(y_true, y_pred)\n",
    "    print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_dim = 300\n",
    "hidden_dim = 150\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using BiLSTM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 23.1795, Validation Loss: 25.8372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 21.5389, Validation Loss: 24.1443\n",
      "Epoch 3 - Training Loss: 21.3365, Validation Loss: 23.5341\n",
      "Epoch 4 - Training Loss: 21.1835, Validation Loss: 23.1461\n",
      "Epoch 5 - Training Loss: 21.0966, Validation Loss: 22.6942\n",
      "Epoch 6 - Training Loss: 21.1763, Validation Loss: 22.5892\n",
      "Epoch 7 - Training Loss: 21.1276, Validation Loss: 24.2814\n",
      "Epoch 8 - Training Loss: 20.9908, Validation Loss: 24.8942\n",
      "Epoch 9 - Training Loss: 20.9868, Validation Loss: 23.5566\n",
      "Epoch 10 - Training Loss: 20.9327, Validation Loss: 22.9056\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMRegression(input_dim*2, hidden_dim, num_layers=2, dropout_prob = 0.3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_losses, val_losses = train(model, optimizer, num_epochs, trainloader,valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.01\n",
      "Pearson correlation coefficient: 0.02\n",
      "Pearson correlation coefficient: -0.01\n"
     ]
    }
   ],
   "source": [
    "print_values(model, trainloader, y_train)\n",
    "print_values(model, valloader, y_val)\n",
    "print_values(model, testloader,  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using GRU Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 22.3771, Validation Loss: 24.1071\n",
      "Epoch 2 - Training Loss: 21.3319, Validation Loss: 22.5483\n",
      "Epoch 3 - Training Loss: 21.1420, Validation Loss: 23.0312\n",
      "Epoch 4 - Training Loss: 21.0430, Validation Loss: 22.6348\n",
      "Epoch 5 - Training Loss: 20.9786, Validation Loss: 23.8589\n",
      "Epoch 6 - Training Loss: 20.8826, Validation Loss: 22.6966\n",
      "Epoch 7 - Training Loss: 20.9325, Validation Loss: 22.6483\n",
      "Epoch 8 - Training Loss: 20.8470, Validation Loss: 24.4998\n",
      "Epoch 9 - Training Loss: 20.7392, Validation Loss: 23.0386\n",
      "Epoch 10 - Training Loss: 20.7176, Validation Loss: 23.4805\n"
     ]
    }
   ],
   "source": [
    "model = GRURegression(input_dim*2, hidden_dim, num_layers=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_losses, val_losses = train(model, optimizer, num_epochs, trainloader,valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.01\n",
      "Pearson correlation coefficient: 0.01\n",
      "Pearson correlation coefficient: 0.06\n"
     ]
    }
   ],
   "source": [
    "print_values(model, trainloader, y_train)\n",
    "print_values(model, valloader, y_val)\n",
    "print_values(model, testloader,  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using BiLSTM Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 22.7185, Validation Loss: 24.5618\n",
      "Epoch 2 - Training Loss: 21.4504, Validation Loss: 25.5685\n",
      "Epoch 3 - Training Loss: 21.0165, Validation Loss: 22.6655\n",
      "Epoch 4 - Training Loss: 21.0623, Validation Loss: 22.6399\n",
      "Epoch 5 - Training Loss: 21.0307, Validation Loss: 22.5291\n",
      "Epoch 6 - Training Loss: 20.7555, Validation Loss: 22.5205\n",
      "Epoch 7 - Training Loss: 20.7062, Validation Loss: 23.3519\n",
      "Epoch 8 - Training Loss: 20.8818, Validation Loss: 23.2409\n",
      "Epoch 9 - Training Loss: 20.7222, Validation Loss: 23.3023\n",
      "Epoch 10 - Training Loss: 21.0281, Validation Loss: 24.5475\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMAttention(input_dim*2, hidden_dim, num_layers=2, attention_dim=600)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_losses, val_losses = train(model, optimizer, num_epochs, trainloader,valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.00\n",
      "Pearson correlation coefficient: -0.03\n",
      "Pearson correlation coefficient: 0.03\n"
     ]
    }
   ],
   "source": [
    "print_values(model, trainloader, y_train)\n",
    "print_values(model, valloader, y_val)\n",
    "print_values(model, testloader,  y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
