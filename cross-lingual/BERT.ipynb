{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        encoded_dict = tokenizer.encode_plus(sentences[i], sentences[i+1], add_special_tokens=True, max_length=max_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt', truncation_strategy='longest_first')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Lingual Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "trainpath = './data/train-en-es.csv'\n",
    "testpath = './data/test-en-es.csv'\n",
    "valpath = './data/validation-en-es.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath)\n",
    "testdata = pd.read_csv(testpath)\n",
    "valdata = pd.read_csv(valpath)\n",
    "\n",
    "traindata['similarity_score'] = traindata['similarity_score'].apply(lambda x: (x)/5.0)\n",
    "testdata['similarity_score'] = testdata['similarity_score'].apply(lambda x: (x)/5.0)\n",
    "valdata['similarity_score'] = valdata['similarity_score'].apply(lambda x: (x)/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARN_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "# compute the sequence length using 95% samples logic\n",
    "lengths = []\n",
    "for _, row in traindata.iterrows():\n",
    "    lengths.append(len(row['sentence1']))\n",
    "    lengths.append(len(row['sentence2']))\n",
    "\n",
    "lengths.sort()\n",
    "MAX_LEN = lengths[int(0.95*len(lengths))]\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=1,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.10\n"
     ]
    }
   ],
   "source": [
    "model_untrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "x_train = []\n",
    "for _, row in traindata.iterrows():\n",
    "    x_train.append(row['sentence1'])\n",
    "    x_train.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_train, tokenizer, MAX_LEN)\n",
    "y_train = torch.tensor(traindata['similarity_score'], dtype=torch.float)\n",
    "\n",
    "trainset = TensorDataset(input_ids, attention_masks, token_type_ids, y_train)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(trainloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model_untrained(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.07\n"
     ]
    }
   ],
   "source": [
    "x_val = []\n",
    "for _, row in valdata.iterrows():\n",
    "    x_val.append(row['sentence1'])\n",
    "    x_val.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_val, tokenizer, MAX_LEN)\n",
    "y_val = torch.tensor(valdata['similarity_score'], dtype=torch.float)\n",
    "valset = TensorDataset(input_ids, attention_masks, token_type_ids, y_val)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(valloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model_untrained(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.05\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "for _, row in testdata.iterrows():\n",
    "    x_test.append(row['sentence1'])\n",
    "    x_test.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_test, tokenizer, MAX_LEN)\n",
    "y_test = torch.tensor(testdata['similarity_score'], dtype=torch.float)\n",
    "\n",
    "testset = TensorDataset(input_ids, attention_masks, token_type_ids, y_test)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(testloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model_untrained(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 0.08565235095512536\n",
      "Epoch: 1\tLoss: 0.08529773299685783\n",
      "Epoch: 2\tLoss: 0.08127135331225065\n",
      "Epoch: 3\tLoss: 0.07632533558126953\n",
      "Epoch: 4\tLoss: 0.0719201358138687\n",
      "Epoch: 5\tLoss: 0.06540682605571217\n",
      "Epoch: 6\tLoss: 0.0581478969193995\n",
      "Epoch: 7\tLoss: 0.05175956537843578\n",
      "Epoch: 8\tLoss: 0.045359986746269794\n",
      "Epoch: 9\tLoss: 0.0412524215100954\n",
      "Epoch: 10\tLoss: 0.03538322852820986\n",
      "Epoch: 11\tLoss: 0.03284739189677768\n",
      "Epoch: 12\tLoss: 0.03021565057699465\n",
      "Epoch: 13\tLoss: 0.02750972482479281\n",
      "Epoch: 14\tLoss: 0.025633901060144935\n",
      "Epoch: 15\tLoss: 0.024102775928460888\n",
      "Epoch: 16\tLoss: 0.022011307658006747\n",
      "Epoch: 17\tLoss: 0.020806827921316856\n",
      "Epoch: 18\tLoss: 0.020534456086655457\n",
      "Epoch: 19\tLoss: 0.019225979750303344\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE, betas=[0.5, 0.99])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "x_train = []\n",
    "for _, row in traindata.iterrows():\n",
    "    x_train.append(row['sentence1'])\n",
    "    x_train.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_train, tokenizer, MAX_LEN)\n",
    "y_train = torch.tensor(traindata['similarity_score'], dtype=torch.float)\n",
    "\n",
    "trainset = TensorDataset(input_ids, attention_masks, token_type_ids, y_train)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "x_val = []\n",
    "for _, row in valdata.iterrows():\n",
    "    x_val.append(row['sentence1'])\n",
    "    x_val.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_val, tokenizer, MAX_LEN)\n",
    "y_val = torch.tensor(valdata['similarity_score'], dtype=torch.float)\n",
    "valset = TensorDataset(input_ids, attention_masks, token_type_ids, y_val)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "x_test = []\n",
    "for _, row in testdata.iterrows():\n",
    "    x_test.append(row['sentence1'])\n",
    "    x_test.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_test, tokenizer, MAX_LEN)\n",
    "y_test = torch.tensor(testdata['similarity_score'], dtype=torch.float)\n",
    "\n",
    "testset = TensorDataset(input_ids, attention_masks, token_type_ids, y_test)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    t_loss = 0\n",
    "    for _, batch in enumerate(trainloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "    print(f'Epoch: {epoch}\\tLoss: {t_loss / len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.81\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(trainloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred) \n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.14\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(valloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.16\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(testloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
