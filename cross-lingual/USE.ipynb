{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 17:25:15.027218: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-08 17:25:15.068613: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 17:25:15.762154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "trainpath = 'data/train-en-es.csv'\n",
    "testpath = 'data/test-en-es.csv'\n",
    "valpath = 'data/validation-en-es.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath)\n",
    "testdata = pd.read_csv(testpath)\n",
    "valdata = pd.read_csv(valpath)\n",
    "\n",
    "traindata['score'] = traindata['similarity_score'].apply(lambda x: (x)/5.0)\n",
    "testdata['score'] = testdata['similarity_score'].apply(lambda x: (x)/5.0)\n",
    "valdata['score'] = valdata['similarity_score'].apply(lambda x: (x)/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 17:25:17.225780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-08 17:25:17.226347: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "modelpath = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\n",
    "use = hub.load(modelpath)\n",
    "\n",
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr\n",
    "\n",
    "def embed(input):\n",
    "    return use(input)\n",
    "\n",
    "def run_sts_benchmark(batch):\n",
    "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sentence1'].tolist())), axis=1)\n",
    "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sentence2'].tolist())), axis=1)\n",
    "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.01\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for batch in np.array_split(traindata, 32):\n",
    "  scores.extend(run_sts_benchmark(batch))\n",
    "\n",
    "corr = pearson_corr(traindata['score'], scores)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.04\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for batch in np.array_split(testdata, 32):\n",
    "  scores.extend(run_sts_benchmark(batch))\n",
    "\n",
    "corr = pearson_corr(testdata['score'], scores)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.01\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for batch in np.array_split(valdata, 32):\n",
    "  scores.extend(run_sts_benchmark(batch))\n",
    "\n",
    "corr = pearson_corr(valdata['score'], scores)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - loss: 0.0938 - mse: 0.0938 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 2/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.0951 - val_mse: 0.0951\n",
      "Epoch 3/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 1s/step - loss: 0.0699 - mse: 0.0699 - val_loss: 0.0982 - val_mse: 0.0982\n",
      "Epoch 4/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 1s/step - loss: 0.0665 - mse: 0.0665 - val_loss: 0.0964 - val_mse: 0.0964\n",
      "Epoch 5/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 1s/step - loss: 0.0635 - mse: 0.0635 - val_loss: 0.1154 - val_mse: 0.1154\n",
      "Epoch 6/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 1s/step - loss: 0.0606 - mse: 0.0606 - val_loss: 0.1000 - val_mse: 0.1000\n",
      "Epoch 7/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 1s/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.1016 - val_mse: 0.1016\n",
      "Epoch 8/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - loss: 0.0476 - mse: 0.0476 - val_loss: 0.1041 - val_mse: 0.1041\n",
      "Epoch 9/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.1049 - val_mse: 0.1049\n",
      "Epoch 10/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 1s/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.1065 - val_mse: 0.1065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79964e8b6ed0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class USELayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(USELayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.use = hub.load(modelpath)\n",
    "        super(USELayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.use(inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 512)\n",
    "\n",
    "input1 = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "input2 = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embedding1 = USELayer()(input1)\n",
    "embedding2 = USELayer()(input2)\n",
    "\n",
    "concatenated = tf.keras.layers.Concatenate()([embedding1, embedding2])\n",
    "dense1 = tf.keras.layers.Dense(512, activation='relu')(concatenated)\n",
    "output = tf.keras.layers.Dense(1, activation='linear')(dense1)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "x_train = [traindata['sentence1'], traindata['sentence2']]\n",
    "y_train = np.array(traindata['score'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 952ms/step\n",
      "Pearson correlation coefficient: 0.6725686443548434\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train).flatten()\n",
    "corr = pearsonr(y_train, y_pred)\n",
    "print(\"Pearson correlation coefficient:\", corr.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 968ms/step\n",
      "Pearson correlation coefficient: 0.11662998527877845\n"
     ]
    }
   ],
   "source": [
    "x_val = [valdata['sentence1'], valdata['sentence2']]\n",
    "y_val = np.array(valdata['score'])\n",
    "\n",
    "y_pred = model.predict(x_val).flatten()\n",
    "corr = pearsonr(y_val, y_pred)\n",
    "print(\"Pearson correlation coefficient:\", corr.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 845ms/step\n",
      "Pearson correlation coefficient: 0.19142071585823445\n"
     ]
    }
   ],
   "source": [
    "x_test = [testdata['sentence1'], testdata['sentence2']]\n",
    "y_test = np.array(testdata['score'])\n",
    "\n",
    "y_pred = model.predict(x_test).flatten()\n",
    "corr = pearsonr(y_test, y_pred)\n",
    "print(\"Pearson correlation coefficient:\", corr.statistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
