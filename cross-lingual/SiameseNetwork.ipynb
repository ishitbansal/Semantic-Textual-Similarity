{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese BiLSTM Neural Network with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(modelpath, binary=True)\n",
    "word2idx = {word: i for i, word in enumerate(model.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Lingual Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "train_path = './data/train-en-es.csv'\n",
    "test_path = './data/test-en-es.csv'\n",
    "val_path = './data/validation-en-es.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "val_data = pd.read_csv(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors_es(file_path):\n",
    "    word_to_vec = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            word_to_vec[word] = vector\n",
    "    return word_to_vec\n",
    "\n",
    "\n",
    "modelpath = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(modelpath, binary=True)\n",
    "word2idx = {word: i for i, word in enumerate(model.index_to_key)}\n",
    "model_spanish_path = \"./SBW-vectors-300-min5.txt\"\n",
    "word2idx_es = load_word_vectors_es(model_spanish_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = {}\n",
    "vocab_es = {}\n",
    "j = 0   \n",
    "sentences_1 = train_data['sentence1'].apply(eval)\n",
    "sentences_2 = train_data['sentence2'].apply(eval)\n",
    "\n",
    "for i in range(len(sentences_1)):\n",
    "    for word in sentences_1[i]:\n",
    "        if word not in vocab_en and word in model.key_to_index:\n",
    "            vocab_en[word] = j\n",
    "            j += 1\n",
    "    for word in sentences_2[i]:\n",
    "        if word not in vocab_es and word in word2idx_es:\n",
    "            vocab_es[word] = j\n",
    "            j += 1\n",
    "\n",
    "\n",
    "word2idx_dataset = {}\n",
    "for i in list(vocab_en.keys()):\n",
    "    word2idx_dataset[vocab_en[i]] = word2idx[i]\n",
    "word2idx_dataset['unk'] = len(word2idx_dataset)\n",
    "word_indices = word2idx_dataset.values()\n",
    "dataset_embed_matrix = model.vectors[np.array(list(word_indices))]\n",
    "\n",
    "\n",
    "word2idx_es_dataset = {}\n",
    "for i in list(vocab_es.keys()):\n",
    "    word2idx_es_dataset[vocab_es[i]] = word2idx_es[i]\n",
    "word2idx_es_dataset['unk'] = len(word2idx_es_dataset)\n",
    "word_indices_es = list(word2idx_es_dataset.values())\n",
    "dataset_embed_matrix_es = word_indices_es\n",
    "# dataset_embed_matrix_es = model_spanish.vectors[np.array(list(word_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self, sentences1, sentences2, scores, word2idx, word2idx_es):\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.scores = scores\n",
    "        self.word2idx = word2idx\n",
    "        self.word2idx_es = word2idx_es\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.sentences1), len(self.sentences2))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unk_token = self.word2idx['unk']\n",
    "        unk_token_es = self.word2idx_es['unk']-1\n",
    "        sentence1 = self.sentences1[idx]\n",
    "        sentence2 = self.sentences2[idx]\n",
    "        score = self.scores[idx]\n",
    "        seq1 = [self.word2idx[word] if word in self.word2idx else unk_token for word in sentence1]\n",
    "        seq2 = [self.word2idx_es[word] if word in self.word2idx_es else unk_token_es for word in sentence2]\n",
    "        return seq1, seq2, score\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        sequences1, sequences2, scores = zip(*batch)\n",
    "        padded_seqs1 = pad_sequence([torch.LongTensor(seq) for seq in sequences1], batch_first=True, padding_value=0)\n",
    "        padded_seqs2 = pad_sequence([torch.LongTensor(seq) for seq in sequences2], batch_first=True, padding_value=0)\n",
    "        return padded_seqs1, padded_seqs2, torch.LongTensor(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence1'] = train_data['sentence1'].apply(eval)\n",
    "train_data['sentence2'] = train_data['sentence2'].apply(eval)\n",
    "val_data['sentence1'] = val_data['sentence1'].apply(eval)\n",
    "val_data['sentence2'] = val_data['sentence2'].apply(eval)\n",
    "test_data['sentence1'] = test_data['sentence1'].apply(eval)\n",
    "test_data['sentence2'] = test_data['sentence2'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset1(train_data['sentence1'], train_data['sentence2'], train_data['similarity_score'], word2idx_dataset, word2idx_es_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "val_dataset = CustomDataset1(val_data['sentence1'], val_data['sentence2'], val_data['similarity_score'], word2idx_dataset, word2idx_es_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=val_dataset.collate_fn)\n",
    "test_dataset = CustomDataset1(test_data['sentence1'], test_data['sentence2'], test_data['similarity_score'], word2idx_dataset, word2idx_es_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, embedding_dim, embd_matrix, embd_matrix_es, dropout=0.2):\n",
    "        super(SiameseBiLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embd_matrix = embd_matrix\n",
    "        self.embd_matrix_es = embd_matrix_es\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(len(embd_matrix), embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.from_numpy(self.embd_matrix))\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "\n",
    "        self.word_embeddings_es = nn.Embedding(len(embd_matrix_es), embedding_dim)\n",
    "        self.word_embeddings_es.weight = nn.Parameter(torch.from_numpy(self.embd_matrix_es))\n",
    "        self.word_embeddings_es.weight.requires_grad = False\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.attention_softmax = nn.Softmax(dim=1)\n",
    "        self.fc = nn.Linear(hidden_size * 4, 1)  # 4 because we concatenate forward and backward hidden states of both LSTMs\n",
    "\n",
    "\n",
    "    def forward_once(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        attention_weights = self.attention_softmax(self.attention_fc(lstm_out))\n",
    "        lstm_out = lstm_out * attention_weights\n",
    "        lstm_out = lstm_out.sum(dim=1)\n",
    "        return lstm_out\n",
    "    \n",
    "\n",
    "    def forward_once_es(self, sentence):\n",
    "        embeds = self.word_embeddings_es(sentence)\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        attention_weights = self.attention_softmax(self.attention_fc(lstm_out))\n",
    "        lstm_out = lstm_out * attention_weights\n",
    "        lstm_out = lstm_out.sum(dim=1)\n",
    "        return lstm_out\n",
    "    \n",
    "\n",
    "    def forward(self, sentence1, sentence2):\n",
    "        # Process sentence 1\n",
    "        output1 = self.forward_once(sentence1)\n",
    "        # Process sentence 2\n",
    "        output2 = self.forward_once_es(sentence2)\n",
    "        # Concatenate outputs of both LSTMs\n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "        # Pass through similarity scoring layer\n",
    "        similarity_score = torch.sigmoid(self.fc(concatenated))\n",
    "        return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\tTraining Loss = 0.0052002502689881625\n",
      "Epoch = 0\tValidation Loss = 0.0001190970724564977\n",
      "Epoch = 1\tTraining Loss = 0.005161643595897876\n",
      "Epoch = 1\tValidation Loss = 0.00015328056178987026\n",
      "Epoch = 2\tTraining Loss = 0.005144361186248361\n",
      "Epoch = 2\tValidation Loss = 0.00016354123363271356\n",
      "Epoch = 3\tTraining Loss = 0.005150131183190592\n",
      "Epoch = 3\tValidation Loss = 7.925344107206911e-05\n",
      "Epoch = 4\tTraining Loss = 0.0051301934193073925\n",
      "Epoch = 4\tValidation Loss = 0.00015763085684739053\n",
      "Epoch = 5\tTraining Loss = 0.00513353794398443\n",
      "Epoch = 5\tValidation Loss = 8.730412810109556e-05\n",
      "Epoch = 6\tTraining Loss = 0.005127392474683103\n",
      "Epoch = 6\tValidation Loss = 0.00015541286848019809\n",
      "Epoch = 7\tTraining Loss = 0.005122537403433069\n",
      "Epoch = 7\tValidation Loss = 0.00012636622705031186\n",
      "Epoch = 8\tTraining Loss = 0.005132778882876669\n",
      "Epoch = 8\tValidation Loss = 0.00010160348756471649\n",
      "Epoch = 9\tTraining Loss = 0.005116694439497382\n",
      "Epoch = 9\tValidation Loss = 0.00015398918185383081\n"
     ]
    }
   ],
   "source": [
    "# Define model and optimizer\n",
    "model1 = SiameseBiLSTM(hidden_size=50, num_layers=2, embedding_dim=300, embd_matrix = dataset_embed_matrix, embd_matrix_es=np.array(dataset_embed_matrix_es[:-1]))\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "model1.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for sentence1, sentence2, score in train_loader:\n",
    "        sentence1_tensor = sentence1\n",
    "        sentence2_tensor = sentence2\n",
    "        score_tensor = torch.tensor(score, dtype=torch.float)/5\n",
    "        optimizer.zero_grad()\n",
    "        output = model1(sentence1_tensor, sentence2_tensor)\n",
    "        loss = criterion(output.squeeze(), score_tensor.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch = {epoch}\\tTraining Loss = {train_loss/len(train_data)}\")\n",
    "    \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence1, sentence2, score in val_loader:\n",
    "            sentence1_tensor = sentence1\n",
    "            sentence2_tensor = sentence2\n",
    "            score_tensor = torch.tensor(score, dtype=torch.float)/5\n",
    "            outputs = model1(sentence1_tensor, sentence2_tensor)\n",
    "            val_loss = criterion(outputs.squeeze(), score_tensor.squeeze())\n",
    "            val_loss += val_loss.item()\n",
    "    \n",
    "    print(f\"Epoch = {epoch}\\tValidation Loss = {val_loss/len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 5.5655\n"
     ]
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_labels = []\n",
    "model1.eval()\n",
    "for train_sentence1, train_sentence2, train_score in train_loader:\n",
    "    train_sentence1_tensor = train_sentence1\n",
    "    train_sentence2_tensor = train_sentence2\n",
    "    train_score_tensor = torch.tensor(train_score, dtype=torch.float)/5.0\n",
    "    train_output = model1(train_sentence1_tensor, train_sentence2_tensor)\n",
    "    train_predictions.extend(train_output.tolist())\n",
    "    train_labels.extend(train_score)\n",
    "train_predictions = np.array(train_predictions)\n",
    "train_labels = np.array(train_labels)\n",
    "train_mse = mean_squared_error(train_labels, train_predictions)\n",
    "print('Train MSE: {:.4f}'.format(train_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MSE: 4.4869\n"
     ]
    }
   ],
   "source": [
    "val_predictions = []\n",
    "val_labels = []\n",
    "model1.eval()\n",
    "for val_sentence1, val_sentence2, val_score in val_loader:\n",
    "    val_sentence1_tensor = val_sentence1\n",
    "    val_sentence2_tensor = val_sentence2\n",
    "    val_score_tensor = torch.tensor(val_score, dtype=torch.float)/5.0\n",
    "    val_output = model1(val_sentence1_tensor, val_sentence2_tensor)\n",
    "    val_predictions.extend(val_output.tolist())\n",
    "    val_labels.extend(val_score)\n",
    "val_predictions = np.array(val_predictions)\n",
    "val_labels = np.array(val_labels)\n",
    "val_mse = mean_squared_error(val_labels, val_predictions)\n",
    "print('Val MSE: {:.4f}'.format(val_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 5.6714\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "test_labels = []\n",
    "model1.eval()\n",
    "for test_sentence1, test_sentence2, test_score in test_loader:\n",
    "    test_sentence1_tensor = test_sentence1\n",
    "    test_sentence2_tensor = test_sentence2\n",
    "    test_score_tensor = torch.tensor(test_score, dtype=torch.float)/5.0\n",
    "    test_output = model1(test_sentence1_tensor, test_sentence2_tensor)\n",
    "    test_predictions.extend(test_output.tolist())\n",
    "    test_labels.extend(test_score)\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "test_mse = mean_squared_error(test_labels, test_predictions)\n",
    "print('Test MSE: {:.4f}'.format(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21464418087130838"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pearson_corr(train_labels, train_predictions.ravel())\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09902122183083445"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pearson_corr(val_labels, val_predictions.ravel())\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09840476343050476"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pearson_corr(test_labels, test_predictions.ravel())\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
