{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = 'data/train.csv'\n",
    "testpath = 'data/test.csv'\n",
    "valpath = 'data/validation.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath)\n",
    "testdata = pd.read_csv(testpath)\n",
    "valdata = pd.read_csv(valpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained Word2Vec model <br />\n",
    "Downloaded from [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(modelpath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_embedding(sentences):\n",
    "    sentence_embedding=[]\n",
    "    for sentence in sentences:\n",
    "        words = sentence\n",
    "        unk_token = \"unk\"\n",
    "        words = [word if word in model.key_to_index else unk_token for word in words]\n",
    "        if len(words) == 0:\n",
    "            words = [\"unk\"]\n",
    "        embeddings = [model[word] for word in words]\n",
    "        embedding = np.mean(embeddings, axis=0)\n",
    "        sentence_embedding.append(embedding)\n",
    "    return np.array(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = get_sentences_embedding(traindata['sentence1'].apply(eval))\n",
    "x_train2 = get_sentences_embedding(traindata['sentence2'].apply(eval))\n",
    "y_train = list(traindata['score'])\n",
    "\n",
    "x_val1 = get_sentences_embedding(valdata['sentence1'].apply(eval))\n",
    "x_val2 = get_sentences_embedding(valdata['sentence2'].apply(eval))\n",
    "y_val = list(valdata['score'])\n",
    "\n",
    "test_x1 = get_sentences_embedding(testdata['sentence1'].apply(eval))\n",
    "test_x2 = get_sentences_embedding(testdata['sentence2'].apply(eval))\n",
    "y_test = list(testdata['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson coefficient: 0.684437678796524\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = []\n",
    "human_similarity_scores = []\n",
    "\n",
    "for i in range(len(x_train1)):\n",
    "    embedding1 = x_train1[i]\n",
    "    embedding2 = x_train2[i]    \n",
    "    cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    \n",
    "    cosine_similarities.append(cosine_sim)\n",
    "    human_similarity_scores.append(y_train[i])\n",
    "\n",
    "pearson_corr, _ = pearsonr(cosine_similarities, human_similarity_scores)\n",
    "print(\"Pearson coefficient:\", pearson_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson coefficient: 0.7365135183185004\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = []\n",
    "human_similarity_scores = []\n",
    "\n",
    "for i in range(len(x_val1)):\n",
    "    embedding1 = x_val1[i]\n",
    "    embedding2 = x_val2[i]    \n",
    "    cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    \n",
    "    cosine_similarities.append(cosine_sim)\n",
    "    human_similarity_scores.append(y_val[i])\n",
    " \n",
    "pearson_corr, _ = pearsonr(cosine_similarities, human_similarity_scores)\n",
    "print(\"Pearson coefficient:\", pearson_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson coefficient: 0.6292495507627769\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = []\n",
    "human_similarity_scores = []\n",
    "\n",
    "for i in range(len(test_x1)):\n",
    "    embedding1 = test_x1[i]\n",
    "    embedding2 = test_x2[i]    \n",
    "    cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    \n",
    "    cosine_similarities.append(cosine_sim)\n",
    "    human_similarity_scores.append(y_test[i])\n",
    "\n",
    "pearson_corr, _ = pearsonr(cosine_similarities, human_similarity_scores)\n",
    "print(\"Pearson coefficient:\", pearson_corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
