{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "trainpath = 'data/train.csv'\n",
    "testpath = 'data/test.csv'\n",
    "valpath = 'data/validation.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath)\n",
    "testdata = pd.read_csv(testpath)\n",
    "valdata = pd.read_csv(valpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained Word2Vec model <br />\n",
    "Downloaded from [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(modelpath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_embedding(sentences):\n",
    "    sentence_embedding=[]\n",
    "    for sentence in sentences:\n",
    "        words = sentence\n",
    "        unk_token = \"unk\"\n",
    "        words = [word if word in model.key_to_index else unk_token for word in words]\n",
    "        if len(words) == 0:\n",
    "            words = [\"unk\"]\n",
    "        embeddings = [model[word] for word in words]\n",
    "        embedding = np.mean(embeddings, axis=0)\n",
    "        sentence_embedding.append(embedding)\n",
    "    return np.array(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = get_sentences_embedding(traindata['sentence1'].apply(eval))\n",
    "x_train2 = get_sentences_embedding(traindata['sentence2'].apply(eval))\n",
    "x_train = np.concatenate([x_train1, x_train2], axis=1)\n",
    "y_train = list(traindata['score'])\n",
    "\n",
    "x_val1 = get_sentences_embedding(valdata['sentence1'].apply(eval))\n",
    "x_val2 = get_sentences_embedding(valdata['sentence2'].apply(eval))\n",
    "x_val = np.concatenate([x_val1, x_val2], axis=1)\n",
    "y_val = list(valdata['score'])\n",
    "\n",
    "test_x1 = get_sentences_embedding(testdata['sentence1'].apply(eval))\n",
    "test_x2 = get_sentences_embedding(testdata['sentence2'].apply(eval))\n",
    "test_X = np.concatenate([test_x1, test_x2], axis=1)\n",
    "y_test = list(testdata['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.47\n",
      "Pearson correlation coefficient: 0.16\n",
      "Pearson correlation coefficient: 0.19\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(x_train)\n",
    "corr = pearson_corr(y_train, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))\n",
    "\n",
    "y_pred = reg.predict(x_val)\n",
    "corr = pearson_corr(y_val, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))\n",
    "\n",
    "y_pred = reg.predict(test_X)\n",
    "corr = pearson_corr(y_test, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = torch.tensor(x_train1, dtype=torch.float)\n",
    "x_train2 = torch.tensor(x_train2, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "\n",
    "x_val1 = torch.tensor(x_val1, dtype=torch.float)\n",
    "x_val2 = torch.tensor(x_val2, dtype=torch.float)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "x_test1 = torch.tensor(test_x1, dtype=torch.float)\n",
    "x_test2 = torch.tensor(test_x2, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class GRURegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, attention_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_dim = attention_dim\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = SelfAttention(hidden_dim*2, attention_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        att_weights = self.attention(out)\n",
    "        out = torch.sum(out * att_weights, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, attention_input):\n",
    "        output = self.layer1(attention_input)\n",
    "        output = torch.tanh(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, embeds1, embeds2, scores):\n",
    "        self.embeds1 = embeds1\n",
    "        self.embeds2 = embeds2\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeds1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.embeds1[index], self.embeds2[index], self.scores[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 300 \n",
    "hidden_dim = 150\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "trainset = MyDataset(x_train1, x_train2, y_train)\n",
    "valset = MyDataset(x_val1, x_val2, y_val)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs, trainloader, valloader):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            embeds1, embeds2, scores = batch\n",
    "            output = model(embeds1, embeds2)\n",
    "            loss = loss_fn(output.squeeze(), scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(embeds1)\n",
    "        train_loss /= len(trainloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in valloader:\n",
    "                embeds1, embeds2, scores = batch\n",
    "                val_output = model(embeds1, embeds2)\n",
    "                val_loss += loss_fn(val_output.squeeze(), scores).item() * len(embeds1)\n",
    "            val_loss /= len(valloader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {} - Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, train_loss, val_loss))\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate(model, embeds1, embeds2):\n",
    "    val_output = model(embeds1, embeds2)\n",
    "    val_output = val_output.data.numpy().flatten().tolist()\n",
    "    return val_output\n",
    "\n",
    "def print_values(model, x_1, x_2, y_true):\n",
    "    y_pred = evaluate(model, x_1, x_2)\n",
    "    corr = pearson_corr(y_true, y_pred)\n",
    "    print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using GRU Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 21.4006, Validation Loss: 23.4083\n",
      "Epoch 2 - Training Loss: 19.2976, Validation Loss: 22.5980\n",
      "Epoch 3 - Training Loss: 18.6838, Validation Loss: 23.0561\n",
      "Epoch 4 - Training Loss: 17.7664, Validation Loss: 22.5400\n",
      "Epoch 5 - Training Loss: 16.5726, Validation Loss: 23.2552\n",
      "Epoch 6 - Training Loss: 15.4372, Validation Loss: 21.5421\n",
      "Epoch 7 - Training Loss: 14.0234, Validation Loss: 22.1058\n",
      "Epoch 8 - Training Loss: 12.9557, Validation Loss: 20.8532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 11.8800, Validation Loss: 20.1100\n",
      "Epoch 10 - Training Loss: 10.8930, Validation Loss: 19.3320\n"
     ]
    }
   ],
   "source": [
    "model = GRURegression(input_dim*2, hidden_dim, num_layers=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_losses, val_losses = train(model, optimizer, num_epochs, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.75\n",
      "Pearson correlation coefficient: 0.40\n",
      "Pearson correlation coefficient: 0.37\n"
     ]
    }
   ],
   "source": [
    "print_values(model, x_train1, x_train2, y_train)\n",
    "print_values(model, x_val1, x_val2, y_val)\n",
    "print_values(model, x_test1, x_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using BiLSTM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 22.6845, Validation Loss: 24.1329\n",
      "Epoch 2 - Training Loss: 19.5565, Validation Loss: 22.0196\n",
      "Epoch 3 - Training Loss: 18.7331, Validation Loss: 22.5787\n",
      "Epoch 4 - Training Loss: 17.7128, Validation Loss: 21.8706\n",
      "Epoch 5 - Training Loss: 16.3361, Validation Loss: 20.6676\n",
      "Epoch 6 - Training Loss: 15.0035, Validation Loss: 22.2397\n",
      "Epoch 7 - Training Loss: 13.8024, Validation Loss: 21.1904\n",
      "Epoch 8 - Training Loss: 12.6910, Validation Loss: 21.2946\n",
      "Epoch 9 - Training Loss: 11.4926, Validation Loss: 21.5128\n",
      "Epoch 10 - Training Loss: 10.3919, Validation Loss: 21.1640\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMRegression(input_dim*2, hidden_dim, num_layers=2, dropout_prob = 0.3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_losses, val_losses = train(model, optimizer, num_epochs, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.77\n",
      "Pearson correlation coefficient: 0.36\n",
      "Pearson correlation coefficient: 0.34\n"
     ]
    }
   ],
   "source": [
    "print_values(model, x_train1, x_train2, y_train)\n",
    "print_values(model, x_val1, x_val2, y_val)\n",
    "print_values(model, x_test1, x_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using BiLSTM Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 21.8825, Validation Loss: 22.6637\n",
      "Epoch 2 - Training Loss: 19.3532, Validation Loss: 22.6007\n",
      "Epoch 3 - Training Loss: 18.4088, Validation Loss: 23.6965\n",
      "Epoch 4 - Training Loss: 17.4539, Validation Loss: 21.3850\n",
      "Epoch 5 - Training Loss: 16.2254, Validation Loss: 21.8735\n",
      "Epoch 6 - Training Loss: 14.8872, Validation Loss: 21.9184\n",
      "Epoch 7 - Training Loss: 13.6521, Validation Loss: 20.4485\n",
      "Epoch 8 - Training Loss: 12.4625, Validation Loss: 20.7423\n",
      "Epoch 9 - Training Loss: 11.4247, Validation Loss: 20.5441\n",
      "Epoch 10 - Training Loss: 10.3129, Validation Loss: 21.0346\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMAttention(input_dim*2, hidden_dim, num_layers=2, attention_dim=600)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_losses, val_losses = train(model, optimizer, num_epochs, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.77\n",
      "Pearson correlation coefficient: 0.37\n",
      "Pearson correlation coefficient: 0.37\n"
     ]
    }
   ],
   "source": [
    "print_values(model, x_train1, x_train2, y_train)\n",
    "print_values(model, x_val1, x_val2, y_val)\n",
    "print_values(model, x_test1, x_test2, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
