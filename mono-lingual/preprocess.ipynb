{"cells":[{"cell_type":"code","execution_count":45,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-09T10:28:46.069866Z","iopub.status.busy":"2024-04-09T10:28:46.068971Z","iopub.status.idle":"2024-04-09T10:28:48.815093Z","shell.execute_reply":"2024-04-09T10:28:48.814089Z","shell.execute_reply.started":"2024-04-09T10:28:46.069831Z"},"trusted":true},"outputs":[],"source":["import re\n","import sys\n","import nltk\n","import json\n","import warnings\n","import numpy as np\n","import pandas as pd\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","\n","warnings.filterwarnings('ignore')\n","np.set_printoptions(threshold=sys.maxsize)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.817081Z","iopub.status.busy":"2024-04-09T10:28:48.816699Z","iopub.status.idle":"2024-04-09T10:28:48.938986Z","shell.execute_reply":"2024-04-09T10:28:48.937720Z","shell.execute_reply.started":"2024-04-09T10:28:48.817057Z"},"trusted":true},"outputs":[],"source":["# Load all the data\n","trainpath = 'data/train.jsonl'\n","testpath = 'data/test.jsonl'\n","valpath = 'data/validation.jsonl'\n","\n","traindata = pd.read_json(trainpath, lines=True)\n","testdata = pd.read_json(testpath, lines=True)\n","valdata = pd.read_json(valpath, lines=True)\n","\n","traindata = traindata[['sentence1', 'sentence2', 'score']]\n","testdata = testdata[['sentence1', 'sentence2', 'score']]\n","valdata = valdata[['sentence1', 'sentence2', 'score']]"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.941967Z","iopub.status.busy":"2024-04-09T10:28:48.941184Z","iopub.status.idle":"2024-04-09T10:28:48.949346Z","shell.execute_reply":"2024-04-09T10:28:48.948387Z","shell.execute_reply.started":"2024-04-09T10:28:48.941931Z"},"trusted":true},"outputs":[],"source":["def remove_punctuation(text):\n","    return re.sub(r'[^\\w\\s]', '', text)\n","\n","def remove_number(text):\n","    return re.sub(r'\\d+', 'num', text)\n","\n","def replace_url(text):\n","    return re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'url', text)\n","\n","def replace_hashtags(text):\n","    return re.sub(r'#[a-zA-Z\\d]+', 'hashtag', text)\n","\n","def replace_email(text):\n","    return re.sub(r'[a-zA-Z\\.]+@[a-zA-Z\\.\\d]+', 'email', text)\n","\n","def replace_mentions(text):\n","    return re.sub(r'@[a-zA-Z\\.\\d_]+', 'mention', text)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.952245Z","iopub.status.busy":"2024-04-09T10:28:48.951857Z","iopub.status.idle":"2024-04-09T10:28:48.964017Z","shell.execute_reply":"2024-04-09T10:28:48.963144Z","shell.execute_reply.started":"2024-04-09T10:28:48.952215Z"},"trusted":true},"outputs":[],"source":["# Stop words, Lemmatization and Stemming\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.965752Z","iopub.status.busy":"2024-04-09T10:28:48.965421Z","iopub.status.idle":"2024-04-09T10:28:48.973693Z","shell.execute_reply":"2024-04-09T10:28:48.972771Z","shell.execute_reply.started":"2024-04-09T10:28:48.965728Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    text = remove_punctuation(text)\n","    text = remove_number(text)\n","    text = replace_url(text)\n","    text = replace_hashtags(text)\n","    text = replace_email(text)\n","    text = replace_mentions(text)\n","    # convert to lower case\n","    text = text.lower()\n","    sentence = text.split()\n","    # remove stop words\n","    sentence = [word for word in sentence if word not in stop_words]\n","    # apply lemmatize\n","    sentence = [lemmatizer.lemmatize(word) for word in sentence]\n","    # apply stemming\n","    sentence = [stemmer.stem(word) for word in sentence]\n","    return sentence\n","\n","def process_data(data):\n","    data['sentence1'] = data['sentence1'].apply(lambda x: preprocess_text(x))\n","    data['sentence2'] = data['sentence2'].apply(lambda x: preprocess_text(x))\n","    return data\n","    \n","def get_vocab(data):\n","    vocab = set()\n","    for _, row in data.iterrows():\n","        for word in row['sentence1']:\n","            vocab.add(word)\n","        for word in row['sentence2']:\n","            vocab.add(word)\n","    return vocab"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.975244Z","iopub.status.busy":"2024-04-09T10:28:48.974793Z","iopub.status.idle":"2024-04-09T10:28:49.521666Z","shell.execute_reply":"2024-04-09T10:28:49.520455Z","shell.execute_reply.started":"2024-04-09T10:28:48.975221Z"},"trusted":true},"outputs":[],"source":["traindata = process_data(traindata)\n","testdata = process_data(testdata)\n","valdata = process_data(valdata)\n","\n","# Create vocab and word2idx\n","vocab = get_vocab(traindata)\n","word2idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n","\n","file_path = 'word2idx.json'\n","file_path = 'data/word2idx.json'\n","\n","# # Save the dictionary to a JSON file\n","with open(file_path, 'w') as file:\n","    json.dump(word2idx, file)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:49.523116Z","iopub.status.busy":"2024-04-09T10:28:49.522861Z","iopub.status.idle":"2024-04-09T10:28:49.576470Z","shell.execute_reply":"2024-04-09T10:28:49.575827Z","shell.execute_reply.started":"2024-04-09T10:28:49.523095Z"},"trusted":true},"outputs":[],"source":["# Store the processed datasets\n","traindata.to_csv('data/train.csv', index=False)\n","testdata.to_csv('data/test.csv', index=False)\n","valdata.to_csv('data/validation.csv', index=False)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:49.577785Z","iopub.status.busy":"2024-04-09T10:28:49.577518Z","iopub.status.idle":"2024-04-09T10:28:49.581749Z","shell.execute_reply":"2024-04-09T10:28:49.580976Z","shell.execute_reply.started":"2024-04-09T10:28:49.577764Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["8258\n"]}],"source":["print(len(vocab))"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:49.583311Z","iopub.status.busy":"2024-04-09T10:28:49.582692Z","iopub.status.idle":"2024-04-09T10:28:49.599157Z","shell.execute_reply":"2024-04-09T10:28:49.598470Z","shell.execute_reply.started":"2024-04-09T10:28:49.583286Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                             sentence1  \\\n","0                        [plane, take]   \n","1             [man, play, larg, flute]   \n","2  [man, spread, shrede, chees, pizza]   \n","3            [three, men, play, chess]   \n","4                   [man, play, cello]   \n","\n","                                    sentence2  score  \n","0                          [air, plane, take]   5.00  \n","1                          [man, play, flute]   3.80  \n","2  [man, spread, shred, chees, uncook, pizza]   3.80  \n","3                     [two, men, play, chess]   2.60  \n","4                    [man, seat, play, cello]   4.25  \n"]}],"source":["print(traindata[:5])"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4759065,"sourceId":8066474,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
