{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "trainpath = 'data/train.csv'\n",
    "testpath = 'data/test.csv'\n",
    "valpath = 'data/validation.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath)\n",
    "testdata = pd.read_csv(testpath)\n",
    "valdata = pd.read_csv(valpath)\n",
    "\n",
    "traindata['score'] = traindata['score'].apply(lambda x: (x)/5.0)\n",
    "testdata['score'] = testdata['score'].apply(lambda x: (x)/5.0)\n",
    "valdata['score'] = valdata['score'].apply(lambda x: (x)/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "# compute the sequence length using 95% samples logic\n",
    "lengths = []\n",
    "for _, row in traindata.iterrows():\n",
    "    lengths.append(len(row['sentence1']))\n",
    "    lengths.append(len(row['sentence2']))\n",
    "\n",
    "lengths.sort()\n",
    "MAX_LEN = lengths[int(0.95*len(lengths))]\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        encoded_dict = tokenizer.encode_plus(sentences[i], sentences[i+1], add_special_tokens=True, max_length=max_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt', truncation_strategy='longest_first')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARN_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14891167395b450b869c70a5c5d65758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 0.057592995889070964\n",
      "Epoch: 1\tLoss: 0.039322442406167585\n",
      "Epoch: 2\tLoss: 0.0325561978129877\n",
      "Epoch: 3\tLoss: 0.02672251861852904\n",
      "Epoch: 4\tLoss: 0.022323650737396544\n",
      "Epoch: 5\tLoss: 0.019402472633454536\n",
      "Epoch: 6\tLoss: 0.01727930448897597\n",
      "Epoch: 7\tLoss: 0.014879244353829158\n",
      "Epoch: 8\tLoss: 0.013136854858344628\n",
      "Epoch: 9\tLoss: 0.011161395475371844\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "for _, row in traindata.iterrows():\n",
    "    x_train.append(row['sentence1'])\n",
    "    x_train.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_train, tokenizer, MAX_LEN)\n",
    "y_train = torch.tensor(traindata['score'], dtype=torch.float)\n",
    "\n",
    "trainset = TensorDataset(input_ids, attention_masks, token_type_ids, y_train)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=1,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE, betas=[0.5, 0.99])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    t_loss = 0\n",
    "    for _, batch in enumerate(trainloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "    print(f'Epoch: {epoch}\\tLoss: {t_loss / len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.14\n"
     ]
    }
   ],
   "source": [
    "model_untrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "x_val = []\n",
    "for _, row in valdata.iterrows():\n",
    "    x_val.append(row['sentence1'])\n",
    "    x_val.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_val, tokenizer, MAX_LEN)\n",
    "y_val = torch.tensor(valdata['score'], dtype=torch.float)\n",
    "valset = TensorDataset(input_ids, attention_masks, token_type_ids, y_val)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(valloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model_untrained(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.83\n"
     ]
    }
   ],
   "source": [
    "x_val = []\n",
    "for _, row in valdata.iterrows():\n",
    "    x_val.append(row['sentence1'])\n",
    "    x_val.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_val, tokenizer, MAX_LEN)\n",
    "y_val = torch.tensor(valdata['score'], dtype=torch.float)\n",
    "valset = TensorDataset(input_ids, attention_masks, token_type_ids, y_val)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(valloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.16\n"
     ]
    }
   ],
   "source": [
    "model_untrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "x_test = []\n",
    "for _, row in testdata.iterrows():\n",
    "    x_test.append(row['sentence1'])\n",
    "    x_test.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_test, tokenizer, MAX_LEN)\n",
    "y_test = torch.tensor(testdata['score'], dtype=torch.float)\n",
    "\n",
    "testset = TensorDataset(input_ids, attention_masks, token_type_ids, y_test)\n",
    "testloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(testloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model_untrained(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.82\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "for _, row in testdata.iterrows():\n",
    "    x_test.append(row['sentence1'])\n",
    "    x_test.append(row['sentence2'])\n",
    "\n",
    "input_ids, attention_masks, token_type_ids = convert_sentences_to_features(x_test, tokenizer, MAX_LEN)\n",
    "y_test = torch.tensor(testdata['score'], dtype=torch.float)\n",
    "\n",
    "testset = TensorDataset(input_ids, attention_masks, token_type_ids, y_test)\n",
    "testloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(testloader):\n",
    "        input_ids, attention_masks, _, labels = tuple(t for t in batch)\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend([row[0] for row in outputs[1].tolist()])\n",
    "\n",
    "corr = pearson_corr(y_true, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
