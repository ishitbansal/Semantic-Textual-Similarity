{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese BiLSTM Neural Network with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from scipy.stats import pearsonr\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mono-Lingual Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'\n",
    "val_path = './data/validation.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "val_data = pd.read_csv(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(modelpath, binary=True)\n",
    "word2idx = {word: i for i, word in enumerate(model.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "j = 0\n",
    "sentences_1 = train_data['sentence1'].apply(eval)\n",
    "sentences_2 = train_data['sentence2'].apply(eval)\n",
    "\n",
    "for i in range(len(sentences_1)):\n",
    "    for word in sentences_1[i]:\n",
    "        if word not in vocab and word in model.key_to_index:\n",
    "            vocab[word] = j\n",
    "            j += 1\n",
    "    for word in sentences_2[i]:\n",
    "        if word not in vocab and word in model.key_to_index:\n",
    "            vocab[word] = j\n",
    "            j += 1\n",
    "\n",
    "\n",
    "word2idx_dataset = {}\n",
    "for i in list(vocab.keys()):\n",
    "    word2idx_dataset[vocab[i]] = word2idx[i]\n",
    "word2idx_dataset['unk'] = len(word2idx_dataset)\n",
    "word_indices = word2idx_dataset.values()\n",
    "dataset_embed_matrix = model.vectors[np.array(list(word_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences1, sentences2, scores, word2idx):\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.scores = scores\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.sentences1), len(self.sentences2))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unk_token = self.word2idx['unk']\n",
    "        sentence1 = self.sentences1[idx]\n",
    "        sentence2 = self.sentences2[idx]\n",
    "        score = self.scores[idx]\n",
    "        seq1 = [self.word2idx[word] if word in self.word2idx else unk_token for word in sentence1]\n",
    "        seq2 = [self.word2idx[word] if word in self.word2idx else unk_token for word in sentence2]\n",
    "        return seq1, seq2, score\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        sequences1, sequences2, scores = zip(*batch)\n",
    "        padded_seqs1 = pad_sequence([torch.LongTensor(seq) for seq in sequences1], batch_first=True, padding_value=0)\n",
    "        padded_seqs2 = pad_sequence([torch.LongTensor(seq) for seq in sequences2], batch_first=True, padding_value=0)\n",
    "        return padded_seqs1, padded_seqs2, torch.LongTensor(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence1'] = train_data['sentence1'].apply(eval)\n",
    "train_data['sentence2'] = train_data['sentence2'].apply(eval)\n",
    "val_data['sentence1'] = val_data['sentence1'].apply(eval)\n",
    "val_data['sentence2'] = val_data['sentence2'].apply(eval)\n",
    "test_data['sentence1'] = test_data['sentence1'].apply(eval)\n",
    "test_data['sentence2'] = test_data['sentence2'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_data['sentence1'], train_data['sentence2'], train_data['score'], word2idx_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "val_dataset = CustomDataset(val_data['sentence1'], val_data['sentence2'], val_data['score'], word2idx_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=val_dataset.collate_fn)\n",
    "test_dataset = CustomDataset(test_data['sentence1'], test_data['sentence2'], test_data['score'], word2idx_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, embedding_dim, embd_matrix, dropout=0.2):\n",
    "        super(SiameseBiLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embd_matrix = embd_matrix\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(len(embd_matrix), embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.from_numpy(self.embd_matrix))\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.attention_softmax = nn.Softmax(dim=1)\n",
    "        self.fc = nn.Linear(hidden_size * 4, 1)  # 4 because we concatenate forward and backward hidden states of both LSTMs\n",
    "\n",
    "\n",
    "    def forward_once(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        attention_weights = self.attention_softmax(self.attention_fc(lstm_out))\n",
    "        lstm_out = lstm_out * attention_weights\n",
    "        lstm_out = lstm_out.sum(dim=1)\n",
    "        return lstm_out\n",
    "    \n",
    "\n",
    "    def forward(self, sentence1, sentence2):\n",
    "        output1 = self.forward_once(sentence1)\n",
    "        output2 = self.forward_once(sentence2)\n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "        similarity_score = torch.sigmoid(self.fc(concatenated))\n",
    "        return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\tTraining Loss = 0.0051819710101453425\n",
      "Epoch = 0\tValidation Loss = 0.0001505515247117728\n",
      "Epoch = 1\tTraining Loss = 0.005156482133938553\n",
      "Epoch = 1\tValidation Loss = 0.00011690163955790922\n",
      "Epoch = 2\tTraining Loss = 0.005142274671175061\n",
      "Epoch = 2\tValidation Loss = 0.0001518282515462488\n",
      "Epoch = 3\tTraining Loss = 0.005108761222886301\n",
      "Epoch = 3\tValidation Loss = 0.00014679056766908616\n",
      "Epoch = 4\tTraining Loss = 0.0051102235885413134\n",
      "Epoch = 4\tValidation Loss = 0.0001506203698227182\n",
      "Epoch = 5\tTraining Loss = 0.00509234941889305\n",
      "Epoch = 5\tValidation Loss = 0.00015986915968824178\n",
      "Epoch = 6\tTraining Loss = 0.005103327176789757\n",
      "Epoch = 6\tValidation Loss = 0.000184585849638097\n",
      "Epoch = 7\tTraining Loss = 0.005105185196915592\n",
      "Epoch = 7\tValidation Loss = 9.738941298564896e-05\n",
      "Epoch = 8\tTraining Loss = 0.0051082442042376195\n",
      "Epoch = 8\tValidation Loss = 0.00015721259114798158\n",
      "Epoch = 9\tTraining Loss = 0.005106234658949478\n",
      "Epoch = 9\tValidation Loss = 9.45208448683843e-05\n"
     ]
    }
   ],
   "source": [
    "# Define model and optimizer\n",
    "model1 = SiameseBiLSTM(hidden_size=50, num_layers=2, embedding_dim=300, embd_matrix = dataset_embed_matrix)\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "model1.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for sentence1, sentence2, score in train_loader:\n",
    "        sentence1_tensor = sentence1\n",
    "        sentence2_tensor = sentence2\n",
    "        score_tensor = torch.tensor(score, dtype=torch.float)/5\n",
    "        optimizer.zero_grad()\n",
    "        output = model1(sentence1_tensor, sentence2_tensor)\n",
    "        loss = criterion(output.squeeze(), score_tensor.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch = {epoch}\\tTraining Loss = {train_loss/len(train_data)}\")\n",
    "    \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence1, sentence2, score in val_loader:\n",
    "            sentence1_tensor = sentence1\n",
    "            sentence2_tensor = sentence2\n",
    "            score_tensor = torch.tensor(score, dtype=torch.float)/5\n",
    "            outputs = model1(sentence1_tensor, sentence2_tensor)\n",
    "            val_loss = criterion(outputs.squeeze(), score_tensor.squeeze())\n",
    "            val_loss += val_loss.item()\n",
    "    \n",
    "    print(f\"Epoch = {epoch}\\tValidation Loss = {val_loss/len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 5.5902\n"
     ]
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_labels = []\n",
    "model1.eval()\n",
    "for train_sentence1, train_sentence2, train_score in train_loader:\n",
    "    train_sentence1_tensor = train_sentence1\n",
    "    train_sentence2_tensor = train_sentence2\n",
    "    train_score_tensor = torch.tensor(train_score, dtype=torch.float)/5.0\n",
    "    train_output = model1(train_sentence1_tensor, train_sentence2_tensor)\n",
    "    train_predictions.extend(train_output.tolist())\n",
    "    train_labels.extend(train_score)\n",
    "train_predictions = np.array(train_predictions)\n",
    "train_labels = np.array(train_labels)\n",
    "train_mse = mean_squared_error(train_labels, train_predictions)\n",
    "print('Train MSE: {:.4f}'.format(train_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MSE: 4.5013\n"
     ]
    }
   ],
   "source": [
    "val_predictions = []\n",
    "val_labels = []\n",
    "model1.eval()\n",
    "for val_sentence1, val_sentence2, val_score in val_loader:\n",
    "    val_sentence1_tensor = val_sentence1\n",
    "    val_sentence2_tensor = val_sentence2\n",
    "    val_score_tensor = torch.tensor(val_score, dtype=torch.float)/5.0\n",
    "    val_output = model1(val_sentence1_tensor, val_sentence2_tensor)\n",
    "    val_predictions.extend(val_output.tolist())\n",
    "    val_labels.extend(val_score)\n",
    "val_predictions = np.array(val_predictions)\n",
    "val_labels = np.array(val_labels)\n",
    "val_mse = mean_squared_error(val_labels, val_predictions)\n",
    "print('Val MSE: {:.4f}'.format(val_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 5.7274\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "test_labels = []\n",
    "model1.eval()\n",
    "for test_sentence1, test_sentence2, test_score in test_loader:\n",
    "    test_sentence1_tensor = test_sentence1\n",
    "    test_sentence2_tensor = test_sentence2\n",
    "    test_score_tensor = torch.tensor(test_score, dtype=torch.float)/5.0\n",
    "    test_output = model1(test_sentence1_tensor, test_sentence2_tensor)\n",
    "    test_predictions.extend(test_output.tolist())\n",
    "    test_labels.extend(test_score)\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "test_mse = mean_squared_error(test_labels, test_predictions)\n",
    "print('Test MSE: {:.4f}'.format(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22443890007964687"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pearson_corr(train_labels, train_predictions.ravel())\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1192820668167186"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pearson_corr(val_labels, val_predictions.ravel())\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1243573168444356"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pearson_corr(test_labels, test_predictions.ravel())\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
